{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep learning model training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import random\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from glob import glob\n",
    "from model import u2net \n",
    "import geopandas as gpd \n",
    "from notebooks import config\n",
    "import torch.nn.functional as F\n",
    "from utils.imgShow import imsShow\n",
    "from utils.utils import read_scenes\n",
    "from utils.metrics import oa_binary, miou_binary\n",
    "from utils.dataloader import SceneArraySet, PatchPathSet, PatchPathSet_2\n",
    "from model import unet, deeplabv3plus, deeplabv3plus_mobilev2, u2net\n",
    "from model import unet_att, u2net_att, u2net_att_3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train scenes: 48\n",
      "vali patch 512: 293\n"
     ]
    }
   ],
   "source": [
    "patch_size = 512  ## patch size setting\n",
    "patch_resize = None  ## patch resize setting\n",
    "### traset\n",
    "paths_scene_tra, paths_truth_tra = config.paths_scene_tra, config.paths_truth_tra\n",
    "paths_dem_tra = config.paths_dem_tra\n",
    "# paths_dem_tra = config.paths_dem_adjust_tra\n",
    "print(f'train scenes: {len(paths_scene_tra)}')\n",
    "### valset\n",
    "paths_valset = sorted(glob(f'data/dset/valset/patch_{patch_size}/*'))  ## for model prediction \n",
    "# paths_patch_valset = sorted(glob(f'data/dset/valset/patch_{patch_size}_dem_adjust/*'))\n",
    "print(f'vali patch {patch_size}: {len(paths_valset)}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dataset loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenes_arr, truths_arr = read_scenes(paths_scene_tra, \n",
    "                                     paths_truth_tra, \n",
    "                                     paths_dem_tra) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create dataset instances\n",
    "tra_data = SceneArraySet(scenes_arr=scenes_arr, \n",
    "                         truths_arr=truths_arr, \n",
    "                         patch_size=patch_size,\n",
    "                         patch_resize=patch_resize)\n",
    "val_data = PatchPathSet_2(paths_valset=paths_valset, \n",
    "                          patch_resize=patch_resize)\n",
    "tra_loader = torch.utils.data.DataLoader(tra_data, \n",
    "                                         batch_size=4, \n",
    "                                         shuffle=True, \n",
    "                                         num_workers=10)\n",
    "val_loader = torch.utils.data.DataLoader(val_data, \n",
    "                                         batch_size=4, \n",
    "                                         num_workers=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### check model\n",
    "# model = unet(num_bands=7)\n",
    "# model = unet_att(num_bands=7)\n",
    "# model = u2net(num_bands_b1=6, num_bands_b2=1)\n",
    "model = u2net_att_3(num_bands_b1=6, num_bands_b2=1)\n",
    "# model = deeplabv3plus(num_bands=7)\n",
    "# model = deeplabv3plus_mobilev2(num_bands=7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 512, 512])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.randn(2, 7, 512, 512)  \n",
    "output = model(tensor) \n",
    "print(output.shape) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### create loss and optimizer\n",
    "loss_bce = nn.BCELoss()\n",
    "loss_mse = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)  \n",
    "lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, \\\n",
    "                                      mode='min', factor=0.6, patience=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''------train step------'''\n",
    "def train_step(model, loss_fn, optimizer, x, y):\n",
    "    optimizer.zero_grad()\n",
    "    pred = model(x)\n",
    "    loss = loss_fn(pred, y.float())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    miou = miou_binary(pred=pred, truth=y, device=x.device)\n",
    "    oa = oa_binary(pred=pred, truth=y, device=x.device)\n",
    "    return loss, miou, oa\n",
    "\n",
    "'''------validation step------'''\n",
    "def val_step(model, loss_fn, x, y, patch_size, patch_resize):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pred = model(x.float())\n",
    "        if patch_size > 256:\n",
    "            if patch_resize == 256:\n",
    "                pred = F.interpolate(pred, \n",
    "                                size=(patch_size, patch_size), \n",
    "                                mode='bilinear',\n",
    "                                align_corners=True)\n",
    "            crop_start = (patch_size-256)//2\n",
    "            pred = pred[:, :, crop_start:crop_start+256, crop_start:crop_start+256]  ## crop to inner 256x256 patch\n",
    "        loss = loss_fn(pred, y.float())\n",
    "    miou = miou_binary(pred=pred, truth=y, device=x.device)\n",
    "    oa = oa_binary(pred=pred, truth=y, device=x.device)\n",
    "    return loss, miou, oa\n",
    "\n",
    "'''------train loops------'''\n",
    "def train_loops(model, loss_fn, optimizer, \n",
    "                    tra_loader, val_loader, epoches, \n",
    "                    device, patch_size, patch_resize,\n",
    "                    lr_scheduler=None):\n",
    "    tra_loss_loops, tra_miou_loops, tra_oa_loops = [], [], []\n",
    "    val_loss_loops, val_miou_loops, val_oa_loops = [], [], []\n",
    "    model = model.to(device)\n",
    "    size_tra_loader = len(tra_loader)\n",
    "    size_val_loader = len(val_loader)\n",
    "    for epoch in range(epoches):\n",
    "        start = time.time()\n",
    "        tra_loss, val_loss = 0, 0\n",
    "        tra_miou, val_miou = 0, 0\n",
    "        tra_oa, val_oa = 0, 0\n",
    "        '''-----train the model-----'''\n",
    "        for x_batch, y_batch in tra_loader:\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "            loss, miou, oa = train_step(model=model, loss_fn=loss_fn, \n",
    "                                    optimizer=optimizer, x=x_batch, y=y_batch)\n",
    "            tra_loss += loss.item()\n",
    "            tra_miou += miou.item()\n",
    "            tra_oa += oa.item()\n",
    "        if lr_scheduler:\n",
    "          lr_scheduler.step(tra_loss)    # if using ReduceLROnPlateau\n",
    "        '''----- validation the model: time consuming -----'''\n",
    "        for x_batch, y_batch in val_loader:\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "            loss, miou, oa = val_step(model=model, \n",
    "                                        loss_fn=loss_fn, \n",
    "                                        x=x_batch, \n",
    "                                        y=y_batch, \n",
    "                                        patch_size=patch_size, \n",
    "                                        patch_resize=patch_resize)\n",
    "            val_loss += loss.item()\n",
    "            val_miou += miou.item()\n",
    "            val_oa += oa.item()\n",
    "        ## Accuracy\n",
    "        tra_loss = tra_loss/size_tra_loader\n",
    "        val_loss = val_loss/size_val_loader\n",
    "        tra_miou = tra_miou/size_tra_loader\n",
    "        val_miou = val_miou/size_val_loader\n",
    "        tra_oa = tra_oa/size_tra_loader\n",
    "        val_oa = val_oa/size_val_loader\n",
    "        tra_loss_loops.append(tra_loss); tra_miou_loops.append(tra_miou); tra_oa_loops.append(tra_oa)\n",
    "        val_loss_loops.append(val_loss); val_miou_loops.append(val_miou); val_oa_loops.append(val_oa)\n",
    "        print(f'Ep{epoch+1}: tra-> Loss:{tra_loss:.3f},Oa:{tra_oa:.3f},Miou:{tra_miou:.3f}, '\n",
    "                f'val-> Loss:{val_loss:.3f},Oa:{val_oa:.3f}, Miou:{val_miou:.3f},time:{time.time()-start:.1f}s')\n",
    "        ## show the result\n",
    "        if (epoch+1)%20 == 0:\n",
    "            model.eval()\n",
    "            sam_index = random.randrange(len(val_data))\n",
    "            patch, truth = val_data[sam_index]\n",
    "            patch, truth = torch.unsqueeze(patch.float(), 0).to(device), truth.to(device)\n",
    "            pred = model(patch)\n",
    "            if patch_size > 256:\n",
    "                while patch_resize == 256:\n",
    "                    patch = F.interpolate(patch, \n",
    "                                    size=(patch_size, patch_size), \n",
    "                                    mode='bilinear',\n",
    "                                    align_corners=True)\n",
    "                    pred = F.interpolate(pred, \n",
    "                                    size=(patch_size, patch_size), \n",
    "                                    mode='bilinear',\n",
    "                                    align_corners=True)\n",
    "                    break\n",
    "                crop_start = (patch_size-256)//2                \n",
    "                patch_val = patch[:, :, crop_start:crop_start+256, crop_start:crop_start+256]  ## crop to match the size\n",
    "                pred_val = pred[:, :, crop_start:crop_start+256, crop_start:crop_start+256]  ## crop to match the size\n",
    "            ## convert to numpy and plot\n",
    "            patch = patch[0].to('cpu').detach().numpy().transpose(1,2,0)\n",
    "            pdem = patch[:,:, -1]\n",
    "            patch_val = patch_val[0].to('cpu').detach().numpy().transpose(1,2,0)\n",
    "            pred_val = pred_val[0].to('cpu').detach().numpy()\n",
    "            truth_val = truth.to('cpu').detach().numpy()\n",
    "            imsShow([patch, pdem, patch_val, pred_val, truth_val], \n",
    "                    clip_list = (2,2,2,0,0),\n",
    "                    img_name_list=['input_patch', 'pdem', 'patch_val', 'pred_val', 'truth_val'],                     \n",
    "                    figsize=(15, 3))\n",
    "    metrics = {'tra_loss':tra_loss_loops, 'tra_miou':tra_miou_loops, 'tra_oa': tra_oa_loops, \n",
    "                'val_loss': val_loss_loops, 'val_miou': val_miou_loops, 'val_oa': val_oa_loops}\n",
    "    return metrics \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 64.00 GiB. GPU 1 has a total capacity of 23.53 GiB of which 7.86 GiB is free. Process 1968237 has 4.49 GiB memory in use. Including non-PyTorch memory, this process has 11.14 GiB memory in use. Of the allocated memory 10.65 GiB is allocated by PyTorch, and 45.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m device = torch.device(\u001b[33m'\u001b[39m\u001b[33mcuda:1\u001b[39m\u001b[33m'\u001b[39m) \n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m metrics = \u001b[43mtrain_loops\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m                \u001b[49m\u001b[43mepoches\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m                \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloss_mse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m                \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m                \u001b[49m\u001b[43mtra_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtra_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m                \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m                \u001b[49m\u001b[43mpatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m                \u001b[49m\u001b[43mpatch_resize\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m                \u001b[49m\u001b[43mlr_scheduler\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlr_scheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m                \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m  \n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 48\u001b[39m, in \u001b[36mtrain_loops\u001b[39m\u001b[34m(model, loss_fn, optimizer, tra_loader, val_loader, epoches, device, patch_size, patch_resize, lr_scheduler)\u001b[39m\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m x_batch, y_batch \u001b[38;5;129;01min\u001b[39;00m tra_loader:\n\u001b[32m     47\u001b[39m     x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m     loss, miou, oa = \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     49\u001b[39m \u001b[43m                            \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m=\u001b[49m\u001b[43mx_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[43my_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     50\u001b[39m     tra_loss += loss.item()\n\u001b[32m     51\u001b[39m     tra_miou += miou.item()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 4\u001b[39m, in \u001b[36mtrain_step\u001b[39m\u001b[34m(model, loss_fn, optimizer, x, y)\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtrain_step\u001b[39m(model, loss_fn, optimizer, x, y):\n\u001b[32m      3\u001b[39m     optimizer.zero_grad()\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     pred = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m     loss = loss_fn(pred, y.float())\n\u001b[32m      6\u001b[39m     loss.backward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/glanet-luo/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/glanet-luo/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Develop/dev-luo/GlaNet/model/u2net_att_3.py:188\u001b[39m, in \u001b[36mu2net_att_3.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    184\u001b[39m x2_up = \u001b[38;5;28mself\u001b[39m.up_conv2(x2_up)    \u001b[38;5;66;03m# 64\u001b[39;00m\n\u001b[32m    186\u001b[39m \u001b[38;5;66;03m# x1_att_b1 = self.att_1_b1(x1_b1)  # 32\u001b[39;00m\n\u001b[32m    187\u001b[39m \u001b[38;5;66;03m# x1_att_b2 = self.att_1_b2(x1_b2)  # 32\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m188\u001b[39m x1_att = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcross_att_1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx1_b1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx1_b2\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 32\u001b[39;00m\n\u001b[32m    189\u001b[39m x1_up = torch.cat([\u001b[38;5;28mself\u001b[39m.up(x2_up), x1_att], dim=\u001b[32m1\u001b[39m)   \u001b[38;5;66;03m# 64+32\u001b[39;00m\n\u001b[32m    190\u001b[39m x1_up = \u001b[38;5;28mself\u001b[39m.up_conv3(x1_up)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/glanet-luo/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/glanet-luo/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Develop/dev-luo/GlaNet/model/u2net_att_3.py:89\u001b[39m, in \u001b[36mCrossAttentionFusion.forward\u001b[39m\u001b[34m(self, x_s2, x_dem)\u001b[39m\n\u001b[32m     85\u001b[39m v = \u001b[38;5;28mself\u001b[39m.value(x_dem).view(b, -\u001b[32m1\u001b[39m, h * w)  \u001b[38;5;66;03m# [B, C, N]\u001b[39;00m\n\u001b[32m     87\u001b[39m \u001b[38;5;66;03m# 2. 计算交叉注意力图 (S2 vs DEM)\u001b[39;00m\n\u001b[32m     88\u001b[39m \u001b[38;5;66;03m# 这一步在问：S2的光谱像素与DEM的哪些地形位置最匹配？\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m89\u001b[39m attn = \u001b[38;5;28mself\u001b[39m.softmax(\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbmm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq\u001b[49m\u001b[43m)\u001b[49m * \u001b[38;5;28mself\u001b[39m.scale)  \u001b[38;5;66;03m# [B, N, N]\u001b[39;00m\n\u001b[32m     91\u001b[39m \u001b[38;5;66;03m# 3. 用注意力图加权 DEM 的 Value\u001b[39;00m\n\u001b[32m     92\u001b[39m out = torch.bmm(v, attn).view(b, -\u001b[32m1\u001b[39m, h, w)\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 64.00 GiB. GPU 1 has a total capacity of 23.53 GiB of which 7.86 GiB is free. Process 1968237 has 4.49 GiB memory in use. Including non-PyTorch memory, this process has 11.14 GiB memory in use. Of the allocated memory 10.65 GiB is allocated by PyTorch, and 45.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:1') \n",
    "metrics = train_loops(model=model, \n",
    "                epoches=200,  \n",
    "                loss_fn=loss_mse,  \n",
    "                optimizer=optimizer,  \n",
    "                tra_loader=tra_loader,  \n",
    "                val_loader=val_loader,  \n",
    "                patch_size=patch_size,  \n",
    "                patch_resize=None,  \n",
    "                lr_scheduler=lr_scheduler,  \n",
    "                device=device)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # model saving\n",
    "# model_name = 'u2net'\n",
    "# # model_name = 'unet_att'\n",
    "# # net_name = 'deeplabv3plus'\n",
    "# # # net_name = 'deeplabv3plus_mb2'\n",
    "# # path_save = f'model/trained/patch_{patch_size}/{model_name}_weights_1.pth'\n",
    "# path_save = f'model/trained/{model_name}_{patch_size}/{model_name}_weights_2.pth'\n",
    "# torch.save(model.state_dict(), path_save)     ## save weights of the trained model \n",
    "# # # model.load_state_dict(torch.load(path_save, weights_only=True))  ## load the weights of the trained model\n",
    "# # # ## metrics saving\n",
    "# path_metrics = f'model/trained/{model_name}_{patch_size}/{model_name}_metrics_2.csv'    \n",
    "# # path_metrics = f'model/trained/patch_{patch_size}/{model_name}_metrics_1.csv'    \n",
    "# metrics_df = pd.DataFrame(metrics)\n",
    "# metrics_df.to_csv(path_metrics, index=False, sep=',')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "glanet-luo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
